{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition - Model Training\n",
    "## This notebook will help you:\n",
    "1. Collect hand gesture data from your webcam\n",
    "2. Preprocess the data\n",
    "3. Train a CNN model using TensorFlow\n",
    "4. Save the trained model for real-time application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import mediapipe as mp\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Gesture Classes\n",
    "We'll create 6 different gestures for controlling the mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gesture classes\n",
    "GESTURES = {\n",
    "    0: 'palm',           # Move cursor\n",
    "    1: 'index',          # Left click\n",
    "    2: 'peace',          # Right click\n",
    "    3: 'fist',           # Scroll\n",
    "    4: 'thumb_pinky',    # Open app 1\n",
    "    5: 'okay'            # Open app 2\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(GESTURES)\n",
    "IMG_SIZE = 128\n",
    "SAMPLES_PER_GESTURE = 500  # Collect 500 samples per gesture\n",
    "\n",
    "print(\"Gestures to be trained:\")\n",
    "for key, value in GESTURES.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Collection Function\n",
    "This will capture images from your webcam for each gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gesture_data(gesture_name, gesture_id, num_samples=500):\n",
    "    \"\"\"\n",
    "    Collect hand gesture images from webcam\n",
    "    \"\"\"\n",
    "    # Create directory for this gesture\n",
    "    gesture_dir = f\"../data/raw/{gesture_name}\"\n",
    "    os.makedirs(gesture_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize MediaPipe hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    count = 0\n",
    "    print(f\"\\nCollecting data for gesture: {gesture_name}\")\n",
    "    print(\"Press SPACE to start capturing\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    capturing = False\n",
    "    \n",
    "    while count < num_samples:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Flip frame horizontally for mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process with MediaPipe\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        # Draw hand landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                if capturing:\n",
    "                    # Get bounding box of hand\n",
    "                    h, w, c = frame.shape\n",
    "                    x_coords = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "                    y_coords = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "                    \n",
    "                    x_min = int(min(x_coords) * w) - 20\n",
    "                    x_max = int(max(x_coords) * w) + 20\n",
    "                    y_min = int(min(y_coords) * h) - 20\n",
    "                    y_max = int(max(y_coords) * h) + 20\n",
    "                    \n",
    "                    # Ensure coordinates are within frame\n",
    "                    x_min = max(0, x_min)\n",
    "                    y_min = max(0, y_min)\n",
    "                    x_max = min(w, x_max)\n",
    "                    y_max = min(h, y_max)\n",
    "                    \n",
    "                    # Extract hand region\n",
    "                    hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                    \n",
    "                    if hand_roi.size > 0:\n",
    "                        # Resize to fixed size\n",
    "                        hand_roi = cv2.resize(hand_roi, (IMG_SIZE, IMG_SIZE))\n",
    "                        \n",
    "                        # Save image\n",
    "                        img_path = os.path.join(gesture_dir, f\"{gesture_name}_{count}.jpg\")\n",
    "                        cv2.imwrite(img_path, hand_roi)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # Draw rectangle on original frame\n",
    "                        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display info\n",
    "        status = \"CAPTURING\" if capturing else \"Press SPACE to start\"\n",
    "        cv2.putText(frame, f\"{gesture_name.upper()}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Status: {status}\", (10, 70), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Samples: {count}/{num_samples}\", (10, 110), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow('Data Collection', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(' '):\n",
    "            capturing = not capturing\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands.close()\n",
    "    \n",
    "    print(f\"Collected {count} samples for {gesture_name}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Collect Data for All Gestures\n",
    "**Instructions:**\n",
    "- Make sure you have good lighting\n",
    "- Position your hand clearly in front of the camera\n",
    "- Press SPACE to start/stop capturing\n",
    "- Move your hand slightly while capturing to add variation\n",
    "- Press 'q' to skip to next gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for all gestures\n",
    "print(\"=\"*50)\n",
    "print(\"GESTURE DATA COLLECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for gesture_id, gesture_name in GESTURES.items():\n",
    "    input(f\"\\nPress Enter to start collecting '{gesture_name}' gesture...\")\n",
    "    collect_gesture_data(gesture_name, gesture_id, SAMPLES_PER_GESTURE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Data collection completed!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load all collected gesture images and create dataset\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    data_dir = \"../data/raw\"\n",
    "    \n",
    "    for gesture_id, gesture_name in GESTURES.items():\n",
    "        gesture_path = os.path.join(data_dir, gesture_name)\n",
    "        \n",
    "        if not os.path.exists(gesture_path):\n",
    "            print(f\"Warning: {gesture_path} not found\")\n",
    "            continue\n",
    "        \n",
    "        images = os.listdir(gesture_path)\n",
    "        print(f\"Loading {len(images)} images for {gesture_name}...\")\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(gesture_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:\n",
    "                # Convert BGR to RGB\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # Normalize pixel values\n",
    "                img = img.astype('float32') / 255.0\n",
    "                \n",
    "                X.append(img)\n",
    "                y.append(gesture_id)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Load the data\n",
    "X, y = load_data()\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_categorical = to_categorical(y, NUM_CLASSES)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each gesture\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (gesture_id, gesture_name) in enumerate(GESTURES.items()):\n",
    "    # Find first image of this gesture\n",
    "    idx = np.where(y == gesture_id)[0][0]\n",
    "    axes[i].imshow(X[idx])\n",
    "    axes[i].set_title(f\"{gesture_name.upper()}\", fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/sample_gestures.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample gestures visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Create a CNN model for gesture recognition\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Flatten and Dense Layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Output Layer\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "print(\"Data augmentation configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../models/best_gesture_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=GESTURES.values(),\n",
    "            yticklabels=GESTURES.values())\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "report = classification_report(y_true_classes, y_pred_classes, \n",
    "                               target_names=GESTURES.values())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('../models/gesture_model.h5')\n",
    "print(\"Model saved as 'gesture_model.h5'\")\n",
    "\n",
    "# Save model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../models/model_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Model architecture saved as 'model_architecture.json'\")\n",
    "\n",
    "# Save gesture mapping\n",
    "import json\n",
    "with open('../models/gesture_mapping.json', 'w') as f:\n",
    "    json.dump(GESTURES, f, indent=4)\n",
    "print(\"Gesture mapping saved as 'gesture_mapping.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou can now use this model in the real-time gesture controller!\")\n",
    "print(\"Run the 'gesture_controller.py' file from VS Code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Test Model with Live Camera (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_live():\n",
    "    \"\"\"\n",
    "    Test the trained model with live camera feed\n",
    "    \"\"\"\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    print(\"Testing model with live camera...\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Get hand bounding box\n",
    "                h, w, c = frame.shape\n",
    "                x_coords = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "                y_coords = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "                \n",
    "                x_min = int(min(x_coords) * w) - 20\n",
    "                x_max = int(max(x_coords) * w) + 20\n",
    "                y_min = int(min(y_coords) * h) - 20\n",
    "                y_max = int(max(y_coords) * h) + 20\n",
    "                \n",
    "                x_min = max(0, x_min)\n",
    "                y_min = max(0, y_min)\n",
    "                x_max = min(w, x_max)\n",
    "                y_max = min(h, y_max)\n",
    "                \n",
    "                hand_roi = frame[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                if hand_roi.size > 0:\n",
    "                    # Preprocess for prediction\n",
    "                    hand_roi_resized = cv2.resize(hand_roi, (IMG_SIZE, IMG_SIZE))\n",
    "                    hand_roi_rgb = cv2.cvtColor(hand_roi_resized, cv2.COLOR_BGR2RGB)\n",
    "                    hand_roi_normalized = hand_roi_rgb.astype('float32') / 255.0\n",
    "                    hand_roi_batch = np.expand_dims(hand_roi_normalized, axis=0)\n",
    "                    \n",
    "                    # Predict\n",
    "                    prediction = model.predict(hand_roi_batch, verbose=0)\n",
    "                    gesture_id = np.argmax(prediction)\n",
    "                    confidence = prediction[0][gesture_id]\n",
    "                    \n",
    "                    gesture_name = GESTURES[gesture_id]\n",
    "                    \n",
    "                    # Draw prediction\n",
    "                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"{gesture_name}: {confidence:.2f}\", \n",
    "                               (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                               0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow('Model Test', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands.close()\n",
    "\n",
    "# Uncomment to test\n",
    "# test_model_live()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
